"""Main generator for LLM-based reward and policy functions.

Orchestrates the full pipeline:
1. Build prompt
2. Call GPT
3. Parse response
4. Validate code
5. Save/load generated functions
"""

import os
import json
import pickle
from typing import Optional, Dict, Any, Callable
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime

from .gpt_client import GPTClient, call_gpt
from .prompts import build_generation_prompt, SYSTEM_PROMPT
from .parser import parse_llm_response, validate_generated_code, get_function_source


@dataclass
class GeneratedCode:
    """Container for LLM-generated code and metadata."""
    reward_function: str
    policy_function: str
    full_code: str
    metadata: Optional[Dict[str, Any]] = None
    reasoning: Optional[str] = None
    model: str = ""
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def get_reward_func(self) -> Callable:
        """Compile and return the reward function."""
        namespace = {"np": __import__("numpy")}
        exec(self.reward_function, namespace)
        return namespace.get("compute_reward")
    
    def get_policy_func(self) -> Callable:
        """Compile and return the policy function."""
        namespace = {"np": __import__("numpy")}
        exec(self.policy_function, namespace)
        return namespace.get("compute_prior_policy")


def generate_reward_and_policy(
    task_name: str = "assembly",
    api_key: Optional[str] = None,
    model: str = "gpt-4o",
    temperature: float = 0.7,
    save_path: Optional[str] = None,
    verbose: bool = True,
) -> GeneratedCode:
    """Generate reward and policy functions using GPT.
    
    Args:
        task_name: Name of the task
        api_key: OpenAI API key (or uses env var)
        model: GPT model to use
        temperature: Sampling temperature
        save_path: Path to save generated code (optional)
        verbose: Print progress messages
        
    Returns:
        GeneratedCode with reward and policy functions
    """
    if verbose:
        print(f"Generating reward and policy for task: {task_name}")
        print(f"Using model: {model}")
    
    # Build prompt
    prompt = build_generation_prompt(task_name)
    
    if verbose:
        print(f"Prompt length: {len(prompt)} characters")
        print("Calling GPT...")
    
    # Call GPT
    client = GPTClient(api_key=api_key, model=model)
    response = client.chat(
        prompt=prompt,
        system_prompt=SYSTEM_PROMPT,
        temperature=temperature,
        max_tokens=4096,
    )
    
    if verbose:
        print(f"Response received. Tokens used: {response.usage['total_tokens']}")
        print("Parsing response...")
    
    # Parse response
    parsed = parse_llm_response(response.content)
    
    if parsed["code"] is None:
        raise ValueError("No Python code found in LLM response")
    
    # Validate
    is_valid, error = validate_generated_code(parsed["code"])
    if not is_valid:
        raise ValueError(f"Generated code validation failed: {error}")
    
    if verbose:
        print("Code validated successfully!")
    
    # Extract individual functions
    reward_func = get_function_source(parsed, "compute_reward")
    policy_func = get_function_source(parsed, "compute_prior_policy")
    
    if reward_func is None:
        raise ValueError("compute_reward function not found in generated code")
    if policy_func is None:
        raise ValueError("compute_prior_policy function not found in generated code")
    
    # Create result
    generated = GeneratedCode(
        reward_function=reward_func,
        policy_function=policy_func,
        full_code=parsed["code"],
        metadata=parsed.get("metadata"),
        reasoning=parsed.get("reasoning"),
        model=model,
    )
    
    # Save if path provided
    if save_path:
        save_generated_code(generated, save_path)
        if verbose:
            print(f"Saved to: {save_path}")
    
    return generated


def save_generated_code(generated: GeneratedCode, path: str) -> None:
    """Save generated code to file.
    
    Saves as both pickle (for programmatic loading) and Python (for inspection).
    
    Args:
        generated: GeneratedCode to save
        path: Base path (without extension)
    """
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Save pickle
    pkl_path = path.with_suffix(".pkl")
    with open(pkl_path, "wb") as f:
        pickle.dump(generated, f)
    
    # Save Python file for inspection
    py_path = path.with_suffix(".py")
    with open(py_path, "w") as f:
        f.write(f'"""Generated by LLM on {generated.timestamp}\n')
        f.write(f"Model: {generated.model}\n")
        if generated.reasoning:
            f.write(f"\nReasoning:\n{generated.reasoning}\n")
        f.write('"""\n\n')
        f.write("import numpy as np\n\n")
        f.write(generated.full_code)
    
    # Save metadata JSON
    if generated.metadata:
        json_path = path.with_suffix(".json")
        with open(json_path, "w") as f:
            json.dump({
                "metadata": generated.metadata,
                "model": generated.model,
                "timestamp": generated.timestamp,
            }, f, indent=2)


def load_generated_code(path: str) -> GeneratedCode:
    """Load previously generated code.
    
    Args:
        path: Path to pickle file
        
    Returns:
        GeneratedCode instance
    """
    path = Path(path)
    if not path.suffix:
        path = path.with_suffix(".pkl")
    
    with open(path, "rb") as f:
        return pickle.load(f)


def load_functions_from_file(path: str) -> tuple:
    """Load reward and policy functions from a Python file.
    
    This allows loading manually edited or refined functions.
    
    Args:
        path: Path to Python file
        
    Returns:
        (compute_reward, robot_prior_policy) functions
    """
    import importlib.util
    
    spec = importlib.util.spec_from_file_location("generated_module", path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    
    reward_func = getattr(module, "compute_reward", None)
    policy_func = getattr(module, "robot_prior_policy", None)
    
    if reward_func is None:
        raise ValueError(f"compute_reward not found in {path}")
    if policy_func is None:
        raise ValueError(f"robot_prior_policy not found in {path}")
    
    return reward_func, policy_func


# ============================================================================
# CLI Interface
# ============================================================================

def main():
    """Command-line interface for code generation."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Generate reward and policy functions using GPT"
    )
    parser.add_argument(
        "--task", 
        type=str, 
        default="assembly",
        help="Task name (default: assembly)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o",
        help="GPT model to use (default: gpt-4o)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="generated_functions",
        help="Output path (without extension)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Sampling temperature (default: 0.7)"
    )
    
    args = parser.parse_args()
    
    try:
        generated = generate_reward_and_policy(
            task_name=args.task,
            model=args.model,
            temperature=args.temperature,
            save_path=args.output,
            verbose=True,
        )
        
        print("\n" + "=" * 60)
        print("Generated Functions:")
        print("=" * 60)
        print("\n--- compute_reward ---")
        print(generated.reward_function)
        print("\n--- robot_prior_policy ---")
        print(generated.policy_function)
        
        if generated.metadata:
            print("\n--- Metadata ---")
            print(json.dumps(generated.metadata, indent=2))
            
    except Exception as e:
        print(f"Error: {e}")
        exit(1)


if __name__ == "__main__":
    main()
